# **1**. Custom Render Pipeline

这是关于创建**自定义脚本渲染管线**的系列教程的第一部分。它包括初步创建一个**基本的渲染管线**，我们将在未来对其进行扩展

## 1.1 A new Render Pipeline

在过去，Unity只支持一些内置的方法来渲染。Unity 2018引入了**可编写脚本的渲染管道**——简称RP，这使得我们可以做任何我们想做的事情，同时仍然能够依靠`Unity`来完成基本步骤，如**剔除**。Unity 2018还增加了两个用这种新方法制作的实验性RP：`Lightweight RP` 和`High Definition RP`。在Unity 2019年，`Lightweight RP`不再是实验性的，在Unity 2019.3中被重新命名为`Universal RP`。

Universal RP注定要取代目前的传统RP，成为默认的。我们的想法是，它是一个最适合的RP，也将是相当容易定制的。与其说是定制RP，不如说这个系列将从头开始创建一个完整的RP。

本教程通过一个最小的RP开始，该RP使用正向渲染来绘制`unlit shapes`。一旦成功，我们就可以在后面的教程中扩展管道，增加照明、阴影、不同的渲染方法和更多的高级功能。

### Project Setup

我们将在**线性色彩空间**中工作，但Unity 2019.2仍然使用**伽马空间**作为默认。通过 "编辑/项目设置 "进入`Player`设置，然后进入`player`，然后将 "*Other Settings* "下的**色彩空间**切换为线性。

<img src="C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524095416913-1621822485749.png" alt="image-20210524095416913" style="zoom:67%;" />

### Pipeline Asset

目前，Unity使用**默认的渲染管道**。要用一个**自定义的渲染管道**来替换它，我们首先必须为它创建一个资产类型。我们将使用与Unity`Universal RP`大致相同的文件夹结构。创建一个带有`Runtime`子文件夹的`Custom RP `资产文件夹。将`CustomRenderPipelineAsset`类型的==c#脚本==放入其中。

![image-20210524100102741](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524100102741-1621822482555.png)

该资产类型必须在`UnityEngine.Rendering`命名空间中扩展`RenderPipelineAsset`，`RP Asset`的主要目的是给Unity提供一种方法，来掌握负责渲染的**管道对象实例**。`Asset`本身只是一个**句柄**和一个**存储设置**的地方。我们现在还没有任何设置，所以我们要做的就是**给Unity提供一种方法来获得我们的管道对象实例**。这可以通过重写`CreatePipeline`方法来实现，它应该返回一个`RenderPipeline instance`。但是我们还没有定义一个自定义的RP类型，所以从返回`null`开始。

```c#
using UnityEngine;
using UnityEngine.Rendering;

public class CustomRenderPipelineAsset : RenderPipelineAsset
{
    protected override RenderPipeline CreatePipeline()
    {
        throw new null;
    }
}
```

现在我们需要将这种类型的资产添加到**项目**中。为了实现这一点，添加一个`CreateAssetMenu`属性到`CustomRenderPipelineAsset`。

```c#
[CreateAssetMenu]
public class CustomRenderPipelineAsset : RenderPipelineAsset { … }
```

这就在` Asset / Create`菜单里放了一个条目。让我们整洁一点，把它放在**渲染子菜单**中。我们通过将属性的`menuName`属性设置为`Rendering/Custom Render Pipeline`来做到这一点。这个属性可以直接设置在属性类型之后，在圆括号内。

```c#
[CreateAssetMenu(menuName = "Rendering/Custom Render Pipeline")]
public class CustomRenderPipelineAsset : RenderPipelineAsset { … }
```

使用新的菜单项将资产添加到项目中，然后到`Graphics project settings`中，在`Scriptable Render Pipeline Settings`下选择它。

<img src="C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524101426201-1621822489027.png" alt="image-20210524101426201" style="zoom:50%;" />

替换了**默认的RP**，改变了一些事情。首先，很多选项都从**图形设置**中消失了，这在一个信息面板中提到。第二，我们已经禁用了默认的RP，但没有提供有效的替代物，所以没有东西再被渲染了。游戏窗口、场景窗口和材料预览都不再起作用了。如果你打开`frame debugger`——通过`Window / Analysis / Frame Debugger`——并启用它，会看到在**游戏窗口**中确实没有东西被画出来。

### Render Pipeline Instance

创建一个`CustomRenderPipeline`类，并把它的脚本文件放在与`CustomRenderPipelineAsset`相同的文件夹中。这将是用于我们的资产**返回的RP实例**的类型，因此它必须扩展`RenderPipeline`。

```c++
using UnityEngine;
using UnityEngine.Rendering;

public class CustomRenderPipeline : RenderPipeline {
    protected override void Render (
		ScriptableRenderContext context, Camera[] cameras
	) {}
}
```

让`CustomRenderPipelineAsset.CreatePipeline`返回一个`CustomRenderPipeline`的新实例。这将使我们得到一个有效的、功能性的管道，尽管它还没有渲染任何东西。

```c#
protected override RenderPipeline CreatePipeline () {
	return new CustomRenderPipeline();
}
```



## 1.2 Rendering

每一帧，Unity都会在**RP实例**上调用`Render`。它传递一个`context `结构——提供一个与本地引擎的连接，我们可以用它来进行渲染。它还会传递一个**摄像机数组**，因为场景中可能有多个活跃的摄像机。RP有责任按照所提供的顺序渲染这些摄像机。

### Camera Renderer

每个`camera`都会被独立渲染。因此，与其让`CustomRenderPipeline`渲染所有的摄像机，不如把这个责任交给一个专门渲染一个摄像机的新类。将其命名为`CameraRenderer`，并给它一个公共的**Render方法**，其中有一个`context `和一个相机参数。为了方便起见，我们将这些参数存储起来。

```c#
using UnityEngine;
using UnityEngine.Rendering;

public class CameraRenderer {

	ScriptableRenderContext context;

	Camera camera;

	public void Render (ScriptableRenderContext context, Camera camera) {
		this.context = context;
		this.camera = camera;
	}
}
```

让`CustomRenderPipeline`在创建时创建一个渲染器的实例，然后用它来循环渲染所有摄像机。

```c#
CameraRenderer renderer = new CameraRenderer();

protected override void Render (ScriptableRenderContext context, Camera[] cameras) {
    foreach (Camera camera in cameras) {
    	renderer.Render(context, camera);
    }
}
```

我们的**相机渲染器**与`Universal RP`的`scriptable renderers `大致相当。这种方法可以使我们在未来支持**每个摄像机的不同渲染方法**变得简单，例如一个用于第一人称视角，一个用于3D地图叠加，或者正向与延迟渲染。但现在我们将以同样的方式渲染所有的摄像机。

### Drawing the Skybox

`CameraRenderer.Render`的工作是：**绘制其相机可以看到的所有几何图形**。为了清楚起见，将这一特定的任务隔离在一个单独的`DrawVisibleGeometry`方法中。我们将首先让它绘制**默认的天空盒**，这可以通过在**上下文**`context`中调用`DrawSkybox`，并将相机作为参数来完成。

```c#
public void Render (ScriptableRenderContext context, Camera camera) {
    this.context = context;
    this.camera = camera;

    DrawVisibleGeometry();
}

void DrawVisibleGeometry () {
	context.DrawSkybox(camera);
}
```

这还不能使**天空盒**出现。这是因为我们向**上下文**发出的命令是**缓冲的**。我们必须通过对上下文调用`Submit`来提交排队执行的工作。让我们在`DrawVisibleGeometry`之后调用一个单独的`Submit`方法来做这件事。

```c#
public void Render (ScriptableRenderContext context, Camera camera) {
    this.context = context;
    this.camera = camera;

    DrawVisibleGeometry();
    Submit();
}

void Submit () {
    context.Submit();
}
```

**天空盒**终于出现在游戏和场景窗口中。当你启用它时，你也可以在`frame debugger`中看到它的条目。它被列为`Camera.RenderSkybox`，它下面有一个`Draw Mesh`项，代表实际的绘制调用。这与**游戏窗口的渲染**相对应。帧调试器不会报告在其他窗口的绘制。

![image-20210524110715009](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524110715009.png)

请注意，摄像机的方向目前并不影响天盒的渲染方式。我们将摄像机传递给`DrawSkybox`，但这只是用来确定**是否应该绘制天盒**，这是由摄像机的`clear flags`来控制的。

为了正确地渲染`skybox`以及整个场景，我们必须设置`view-projection matrix`。这个变换矩阵结合了摄像机的位置和方向（==视图矩阵==）以及摄像机的透视或正投影（==投影矩阵==）。它在着色器中被称为==unity_MatrixVP==，是绘制几何图形时使用的着色器属性之一。你可以在`frame debugger`的`ShaderProperties`部分检查这个矩阵（当一个绘制调用被选中时）。

目前，`unity_MatrixVP`矩阵总是相同的。我们必须通过`SetupCameraProperties`方法将**相机的属性**应用于上下文`context`。这将设置矩阵以及其他一些属性。在调用`DrawVisibleGeometry`之前，在一个单独的`Setup`方法中做这个。

```c#
public void Render (ScriptableRenderContext context, Camera camera) {
    this.context = context;
    this.camera = camera;

    Setup();
    DrawVisibleGeometry();
    Submit();
}

void Setup () {
    context.SetupCameraProperties(camera);
}
```

![image-20210524111303784](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524111303784.png)

### Command Buffers

`context`会推迟**实际的渲染**，直到我们提交它。在此之前，我们对它进行**配置**，并向它**添加命令**，以便以后执行。一些任务——比如绘制天空盒——可以通过一个专门的方法来发布，但是其他的命令必须通过一个**单独的命令缓冲区**来间接发布。我们需要这样一个**缓冲区**来绘制场景中的其他几何体。

为了得到一个缓冲区，我们必须创建一个新的`CommandBuffer`对象实例。我们**只需要一个缓冲区**，所以默认为`CameraRenderer`创建一个，并存储它的引用。同时给这个缓冲区一个名字，这样我们就可以在`frame debugger`中识别它。

```c#
const string bufferName = "Render Camera";
CommandBuffer buffer = new CommandBuffer();

CameraRenderer()
{
    buffer.name = bufferName;
}
```

我们可以使用**命令缓冲区**来注入`profiler samples`，这些样本将同时显示在`profiler`和`frame debugger`中。这是通过在适当的地方调用`BeginSample`和`EndSample`来实现的，在我们的例子中，这是在`Setup`和`Submit`的开头。这两个方法必须提供**相同的样本名称**，为此我们将使用**缓冲区的名称**。

```c#
void Setup () {
    buffer.BeginSample(bufferName);
    context.SetupCameraProperties(camera);
}

void Submit () {
    buffer.EndSample(bufferName);
    context.Submit();
}
```

要执行缓冲区，在**上下文**`context`中调用`ExecuteCommandBuffer`，并把**缓冲区**作为一个参数。这是从缓冲区中复制命令，但并不清除它，如果我们想重新使用它，我们必须在事后进行`clear`。因为**执行和清除**总是一起进行的，所以添加一个方法来做这两件事是很方便的。

```c#
void Setup () {
    buffer.BeginSample(bufferName);
    ExecuteBuffer();
    context.SetupCameraProperties(camera);
}

void Submit () {
    buffer.EndSample(bufferName);
    ExecuteBuffer();
    context.Submit();
}

void ExecuteBuffer () {
    context.ExecuteCommandBuffer(buffer);
    buffer.Clear();
}
```

`Camera.RenderSkyBox`样本现在被嵌套在`Render Camera`里面。

![image-20210524112951827](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524112951827.png)

### Clearing the Render Target

我们画的东西最终会被渲染到摄像机的`render target`上，默认情况下是`frame buffer`，但也可以是`render texture`。之前绘制到该目标上的东西仍然存在，这可能会干扰我们现在正在渲染的图像。为了保证正常的渲染，**我们必须清除渲染目标**。这可以通过在**命令缓冲区**上调用`ClearRenderTarget`来实现，它属于`Setup`方法。

`CommandBuffer.ClearRenderTarget`至少需要三个参数。前两个表明**深度**和**颜色数据**是否应该被清除，一般是`true`。第三个参数是用于清除的颜色，对此我们将使用`Color.clear`。

```c++
void Setup () {
    buffer.BeginSample(bufferName);
    buffer.ClearRenderTarget(true, true, Color.clear);
    ExecuteBuffer();
    context.SetupCameraProperties(camera);
}
```

![image-20210524113931411](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524113931411.png)

`frame debugger`现在显示了清除动作的`Draw GL`条目，它显示嵌套在`Render Camera`的一个额外层次中。这是因为`ClearRenderTarget`用**命令缓冲区的名字**将`clear`包裹在一个样本中。可以通过在开始样本之前，进行清除来**摆脱多余的嵌套**。

```c#
void Setup () {
    buffer.ClearRenderTarget(true, true, Color.clear);
    buffer.BeginSample(bufferName);
    //buffer.ClearRenderTarget(true, true, Color.clear);
    ExecuteBuffer();
    context.SetupCameraProperties(camera);
}
```

![image-20210524114722454](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524114722454.png)

**Draw GL条目**表示用`Hidden/InternalClear`着色器绘制**全屏四边形**，该着色器会写入**渲染目标**，**这并不是最有效的清除方式**。使用这种方法是因为我们是在**设置摄像机属性**之前进行清除。**如果我们把这两个步骤的顺序对调一下，就可以得到快速的清除方式**。

```c#
 //设置一些属性，如：视图投影矩阵
 void Setup()
 {
     //设置相机属性，如：视图投影矩阵
     context.SetupCameraProperties(camera);
     //清空RT的残余信息
     buffer.ClearRenderTarget(true, true, Color.clear);
     buffer.BeginSample(bufferName);
     ExecuteBuffer();
 }
```

![image-20210524115020744](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524115020744.png)

现在我们看到`Clear（color+Z+stencil）`，这表明颜色和深度缓冲区都被清除了。

### Culling

我们目前看到的是**天空盒**，但没有看到放在场景中的任何物体。与其绘制每一个物体，我们不如只渲染那些**对摄像机可见的物体**。我们通过从场景中所有带有**渲染器组件的物体**开始，然后==剔除==那些位于**摄像机视锥体**之外的物体。

为了弄清楚哪些物体可以被剔除，**我们需要跟踪多个摄像机设置和矩阵**，为此我们可以使用`ScriptableCullingParameters`结构。我们可以在**摄像机**上调用`TryGetCullingParameters`，而不是自己去填充它。它会返回参数是否被成功获取，因为对于退化的相机设置，它可能会失败。为了获得参数数据，我们必须把它作为一个输出参数，在它前面写上**输出**。在一个单独的`Cull`方法中做到这一点，该方法返回成功或失败。

```c#
bool Cull () {
    ScriptableCullingParameters p
    if (camera.TryGetCullingParameters(out p)) {
    	return true;
    }
    return false;
}
```

> 当一个结构参数被定义为输出参数时，它就像一个==对象引用==，指向内存栈中参数所在的位置。当方法改变参数时，**它影响的是这个值，而不是一个副本**。
>
> out关键字告诉我们，该方法负责正确设置参数，替换之前的值。
>
> Try-get方法是一种常见的方法，它既能表示成功或失败，又能产生一个结果。

当作为一个**输出参数**使用时，可以在**参数列表**中==内联==**变量声明**，所以让我们这样做：

```c++
bool Cull () {
    //ScriptableCullingParameters p
    if (camera.TryGetCullingParameters(out ScriptableCullingParameters p)) {
    	return true;
    }
    	return false;
}
```

在`Render`中的`Setup`之前调用`Cull`，如果失败则中止。

```c#
public void Render (ScriptableRenderContext context, Camera camera) {
    this.context = context;
    this.camera = camera;

    if (!Cull()) {
    	return;
    }

    Setup();
    DrawVisibleGeometry();
    Submit();
}
```

==实际的剔除==是通过在**上下文**`context`中调用`Cull`来完成的，它会产生一个`CullingResults`结构。在这种情况下，我们必须把**剔除参数**作为一个==引用参数==来传递，在它前面写上`ref`。

```c#
CullingResults cullingResults;

	…
	
    bool Cull () {
        if (camera.TryGetCullingParameters(out ScriptableCullingParameters p)) {
            cullingResults = context.Cull(ref p);
            return true;
        }
        return false;
    }
```

> ==ref关键字==的工作原理和`out`一样，只是方法不需要给它分配东西。**调用该方法的人负责正确初始化该值**。所以它可以用于输入，也可以用于输出。
>
> 在这种情况下，ref被用来作为一种优化，以防止传递一个相当大的ScriptableCullingParameters结构的副本。

### Drawing Geometry

一旦我们知道**什么是可见的**，我们就可以继续渲染了。这可以通过在**上下文**`context`中调用`DrawRenderers`来完成，并将**剔除结果**作为一个参数，告诉它要使用**哪些渲染器**。除此之外，我们还必须提供**绘制设置**和**过滤设置**。两者都是结构体：绘图设置（`DrawingSettings`）和过滤设置（`FilteringSettings`），我们最初会使用它们的**默认构造函数**。两者都必须通过**引用**来传递。在绘制天空盒之前，在`DrawVisibleGeometry`中这样做。

```c++
void DrawVisibleGeometry () {
    var drawingSettings = new DrawingSettings();
    var filteringSettings = new FilteringSettings();

    context.DrawRenderers(
    cullingResults, ref drawingSettings, ref filteringSettings
    );

    context.DrawSkybox(camera);
}
```

我们还看不到任何东西，因为我们还必须指出哪种`shader passes`是允许的。由于我们在本教程中只支持`unlit shaders`，我们必须获取`SRPDefaultUnlit pass`的**着色器标签ID**。我们可以执行一次，并将其缓存在一个**静态字段**中。

```c#
static ShaderTagId unlitShaderTagId = new ShaderTagId("SRPDefaultUnlit");
```

将其作为`DrawingSettings`**构造函数**的第一个参数，同时提供一个新的`SortingSettings`结构值。将相机传递给`SortingSettings`的构造函数，因为它被用来确定**是否适用正交或基于距离的排序**。

```c#
void DrawVisibleGeometry () {
    var sortingSettings = new SortingSettings(camera);
    var drawingSettings = new DrawingSettings(
    unlitShaderTagId, sortingSettings
    );
    …
}
```

除此之外，我们还必须指出哪些`render queues`是允许的。将`RenderQueueRange.all`传递给`FilteringSettings`构造函数：

```c#
var filteringSettings = new FilteringSettings(RenderQueueRange.all);
```

![image-20210524131756493](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524131756493.png)

![image-20210524131539740](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524131539740.png)

只有使用`unlit shaders`的**可见对象**被绘制。所有的**绘制调用**都列在帧调试器中，被归入`RenderLoop.Draw`。在**透明物体**上发生了一些奇怪的事情，但让我们先看看这些物体**被绘制的顺序**。

![image-20210524131918477](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524131918477.png)

==绘制顺序是杂乱无章的==。我们可以通过设置`SortingSettings`的**标准属性**来强制执行一个特定的绘制顺序。让我们使用：

```c#
var sortingSettings = new SortingSettings(camera) {
    criteria = SortingCriteria.CommonOpaque
};
```

按照作者的例子（我这里物体数量太少了），现在的话，会先绘制**不透明物体**，然后再绘制**半透明物体**。物体现在是==从前到后绘制的==，这对**不透明的物体**来说是很理想的。如果某个物体最终被画在其他物体后面，**它的隐藏片段就会被跳过**，这就加快了渲染速度。常见的**不透明排序选项**还考虑了其他一些标准，包括**渲染队列**和**材质**。

### Drawing Opaque and Transparent Geometry Separately

` frame debugger`向我们展示了**透明物体的绘制**，但**天空盒**被绘制在**所有不在不透明物体前面的物体**上。天空盒被绘制在不透明几何体之后，所以隐藏片段可以被跳过，**但它却覆盖了透明几何体**。发生这种情况是因为`transparent shaders`不向**深度缓冲区**写入。它们不会隐藏后面的东西。**解决方案是首先绘制不透明的物体，然后是天空盒，最后才是透明物体**。

我们可以通过切换到`RenderQueueRange.opaque`来消除`DrawRenderers`中的透明对象。

```c#
var filteringSettings = new FilteringSettings(RenderQueueRange.opaque);
```

然后在绘制**天空盒**后，再次调用DrawRenderers。但在这之前，将**渲染队列范围**改为`RenderQueueRange.transparent`。同时将排序标准改为`SortingCriteria.CommonTransparent`，并再次设置`DrawSetting`的排序。这样就颠倒了透明物体的绘制顺序。

> 由于**透明物体**不向**深度缓冲区**写入数据，因此将它们从前向后排序对性能没有好处。但是，当透明物体最终在视觉上落后于对方时，它们**必须从后往前画才能正确地混合**。
>
> 不幸的是，从后到前的排序并不能保证正确的混合，因为排序是按对象进行的，而且只基于对象的位置。**相交的**和**大的透明物体**仍然会产生不正确的结果。这可以通过将**几何体**切割成**较小的部分**来解决。

```c#
context.DrawSkybox(camera);

sortingSettings.criteria = SortingCriteria.CommonTransparent;
drawingSettings.sortingSettings = sortingSettings;
filteringSettings.renderQueueRange = RenderQueueRange.transparent;

context.DrawRenderers(
cullingResults, ref drawingSettings, ref filteringSettings
);
```

![image-20210524133054918](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524133054918.png)



## 1.3 Editor Rendering

我们的RP可以正确地绘制出` unlit objects`，但是我们可以做一些事情，来改善在**Unity编辑器**中使用它的体验。

### Drawing Legacy Shaders（绘制非法着色器）

因为我们的管道只支持`unlit shaders passes`，使用其他`pass`的物体不会被渲染，从而不可见。虽然这是正确的，但它掩盖了一个事实：即场景中的一些物体使用了错误的着色器。So let's render them anyway, but separately.

如果有人从一个默认的Unity项目开始，然后切换到我们的RP，**那么他们的场景中可能会有使用错误着色器的对象**。为了涵盖Unity的所有默认着色器，我们必须使用`shaders tag IDs `来表示**Always**、**ForwardBase**、**PrepassBase**、**Vertex**、**VertexLMRGBM**和**VertexLM**通道。在一个**静态数组**中保持对这些的跟踪。

```c#
static ShaderTagId[] legacyShaderTagIds = {
    new ShaderTagId("Always"),
    new ShaderTagId("ForwardBase"),
    new ShaderTagId("PrepassBase"),
    new ShaderTagId("Vertex"),
    new ShaderTagId("VertexLMRGBM"),
    new ShaderTagId("VertexLM")
};
```

在单独方法中绘制所有**不支持的着色器**。由于这些是无效的`pass`，结果无论如何都是错误的，所以我们并不关心其他的设置。我们可以通过`FilteringSettings.defaultValue`属性获得默认的过滤设置。

```c#
    public void Render (ScriptableRenderContext context, Camera camera) {
        …

        Setup();
        DrawVisibleGeometry();
        DrawUnsupportedShaders();
        Submit();
    }

…

    void DrawUnsupportedShaders () {
        var drawingSettings = new DrawingSettings(
            legacyShaderTagIds[0], new SortingSettings(camera)
        );
        var filteringSettings = FilteringSettings.defaultValue;
        context.DrawRenderers(
            cullingResults, ref drawingSettings, ref filteringSettings
        );
	}
```

我们可以通过在`drawing settings`上调用`SetShaderPassName`，并将**绘制顺序索引**和**标签**作为参数来绘制多个`pass`。对数组中的所有`pass`都这样做，从第二个开始，因为我们在构建**绘制设置**时已经设置了第一个`pass`。

```c++
var drawingSettings = new DrawingSettings(
	legacyShaderTagIds[0], new SortingSettings(camera)
);
for (int i = 1; i < legacyShaderTagIds.Length; i++) {
	drawingSettings.SetShaderPassName(i, legacyShaderTagIds[i]);
}
```

用==标准着色器==渲染的物体出现了，但它们现在是**纯黑色**的，因为**我们的RP**还没有为它们设置所需的着色器属性。

### Error Material

为了清楚地表明：**哪些对象使用了不支持的着色器**，我们将用Unity的==错误着色器==绘制它们。以该着色器为参数构建一个**新的材质**，我们可以通过调用`Shader.Find`，以`Hidden/InternalErrorShader`字符串为参数来找到它。通过一个静态字段来缓存该材质，然后将其分配给绘制设置的`overrideMaterial`属性。

```c#
static Material errorMaterial;

	…

void DrawUnsupportedShaders () {
    if (errorMaterial == null) {
        errorMaterial =
        new Material(Shader.Find("Hidden/InternalErrorShader"));
    }
    var drawingSettings = new DrawingSettings(
    	legacyShaderTagIds[0], new SortingSettings(camera)
    ) {
    	overrideMaterial = errorMaterial
    };
    …
}
```

![image-20210524135023133](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524135023133.png)

### Partial Class（局部类）

绘制**无效的对象**对于开发来说是很有用的，但并不意味着用于**发布的应用程序**。因此，让我们把`CameraRenderer`的所有**只用于编辑器的代码**放在一个**单独的局部类文件**中。首先，复制原始的CameraRenderer脚本，并将其重命名为`CameraRenderer.Editor`。

然后将原来的`CameraRenderer`变成一个局部类，并从其中移除**标签数组**、**错误材质**和`DrawUnsupportedShaders`方法。

```c#
public partial class CameraRenderer { … }
```

> 什么是部分类`partial classes`？它是一种**将一个类或结构定义分成多个部分**的方法，存储在不同的文件中。**唯一的目的是为了组织代码**。典型的使用情况是将**自动生成的代码**与**手工编写的代码**分开。就编译器而言，这都是同一个类定义的一部分。它们在《对象管理，更复杂的层次》教程中被介绍过。

清理另一个部分类文件，使其只**包含我们从另一个文件中删除的内容**：

```c#
using UnityEngine;
using UnityEngine.Rendering;

partial class CameraRenderer {

	static ShaderTagId[] legacyShaderTagIds = {	… };

	static Material errorMaterial;

	void DrawUnsupportedShaders () { … }
}
```

**编辑器部分的内容**只需要存在于编辑器中，所以让它以`UNITY_EDITOR`为条件。

```c#
partial class CameraRenderer {

#if UNITY_EDITOR

	static ShaderTagId[] legacyShaderTagIds = { … }
	};

	static Material errorMaterial;

	void DrawUnsupportedShaders () { … }

#endif
}
```

然而，此时进行`bulid`会失败，因为另一部分总是包含`DrawUnsupportedShaders`的调用，而现在它只在编辑器中存在。为了解决这个问题，我们**把这个方法也变成局部的**。我们通过在**方法签名**前面加上`partial`来做到这一点，**类似于抽象方法声明**。我们可以在类定义的任何部分这样做，所以让我们把它放在编辑器部分。完整的方法声明也必须用`partial`标记。

```c++
	partial void DrawUnsupportedShaders ();

#if UNITY_EDITOR

	…

	partial void DrawUnsupportedShaders () { … }

#endif
```

现在成功了。编译器将删除**没有完整声明的`partial `方法**的调用。

> 我们可以让无效的对象出现在**开发构建**`development builds`中吗？是的，可以基于`UNITY_EDITOR || DEVELOPMENT_BUILD`来进行条件编译。然后`DrawUnsupportedShaders`也会存在于开发构建中，而在**发布构建**中仍然不会出现。

### Drawing Gizmos（绘制工具？）

目前，我们的RP不画`gizmos`，无论是在场景窗口还是在游戏窗口。我们可以通过调用`UnityEditor.Handles.ShouldRenderGizmos`来检查**是否应该绘制gizmos**。如果是的话，我们必须在**上下文**`context`中调用`DrawGizmos`，将相机作为一个参数，再加上**第二个参数**来指示**应该绘制哪个gizmo子集**。有两个子集，用于**图像效果` image effects`**之前和之后。由于我们目前不支持` image effects`，我们将同时调用这两个子集。在一个新的、编辑器专用的`DrawGizmos`方法中这样做。

```c#
using UnityEditor;
using UnityEngine;
using UnityEngine.Rendering;

partial class CameraRenderer {
	
	partial void DrawGizmos ();

	partial void DrawUnsupportedShaders ();

#if UNITY_EDITOR

	…

	partial void DrawGizmos () {
		if (Handles.ShouldRenderGizmos()) {
			context.DrawGizmos(camera, GizmoSubset.PreImageEffects);
			context.DrawGizmos(camera, GizmoSubset.PostImageEffects);
		}
	}

	partial void DrawUnsupportedShaders () { … }

#endif
}
```

在最后进行绘制：

```c#
public void Render (ScriptableRenderContext context, Camera camera) {
    …

    Setup();
    DrawVisibleGeometry();
    DrawUnsupportedShaders();
    DrawGizmos();
    Submit();
}
```

![image-20210524140839420](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524140839420.png)

### Drawing Unity UI

另一个需要我们注意的是==Unity的游戏内用户接口==。例如，通过`GameObject / UI / Button`添加一个按钮来创建一个简单的`user interface`（UI）。它将显示在**游戏窗口**中，但不会显示在**场景窗口**中。

`frame debugger`告诉我们，**UI是单独渲染的**，不是由我们的RP渲染的。

![image-20210524141151277](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524141151277.png)

至少，当**画布组件**的**渲染模式**被设置为`Screen Space - Overlay`时，情况就是这样，这是默认的。把它改为`Screen Space - Camera`，并使用**主摄像机**作为**它的渲染摄像机**，将使它成为**透明几何的一部分**。

![image-20210524141414196](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524141414196.png)

当用户界面在**场景窗口**中被渲染时，它总是使用**世界空间模式**，**这就是为什么它通常最终会非常大**。但是，虽然我们可以通过**场景窗口**编辑`UI`，但它不会被绘制出来。

在为**场景窗口**渲染时，我们必须明确地将`UI`添加到**世界几何**中，方法是调用`ScriptableRenderContext.EmitWorldGeometryForSceneView`，将相机作为参数。在一个新的编辑器专用的`PrepareForSceneWindow`方法中做这个。当场景相机的`CameraType`属性等于`CameraType.SceneView`时，我们就用**场景相机**进行渲染。

```c#
	partial void PrepareForSceneWindow ();

#if UNITY_EDITOR

	…

	partial void PrepareForSceneWindow () {
		if (camera.cameraType == CameraType.SceneView) {
			ScriptableRenderContext.EmitWorldGeometryForSceneView(camera);
		}
	}
```

因为这可能会给场景添加几何体，所以必须在**剔除**之前完成。

```c#
PrepareForSceneWindow();
if (!Cull()) {
	return;
}
```

![image-20210524141931946](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524141931946.png)



## 1.4 Multiple Cameras

### Two Cameras

每个摄像机都有一个深度值，**默认主摄像机**的深度值为`-1`。**它们会按深度递增的顺序被渲染**。要看到这一点，复制主摄像机，将其重命名为次要摄像机，并将其深度设置为0。给它另一个标签也是个好主意，因为`MainCamera`应该只被**一个摄像机**使用。

场景现在被渲染了两次。结果图像仍然是一样的，因为**渲染目标**在中间被清除了。由于**相邻的具有相同名称的`sample`作用域被合并**，我们最终得到了一个单一的`Render Camera`作用域。

如果**每个摄像机都有自己的作用域**，那就更清楚了。为了使之成为可能，添加一个编辑器专用的`PrepareBuffer`方法，使缓冲区的名称与摄像机的名称相同。

```c#
	partial void PrepareBuffer ();

#if UNITY_EDITOR

	…
	
	partial void PrepareBuffer () {
		buffer.name = camera.name;
	}

#endif
```

```c#
PrepareBuffer();
PrepareForSceneWindow();
```

![image-20210524143355414](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524143355414.png)

### Dealing with Changing Buffer Names

虽然现在`frame debugger`显示每个摄像机，但当我们进入游戏模式时，**Unity的控制台**会充满警告我们BeginSample和EndSample的数量必须匹配的信息。它感到困惑是因为：`sample`和它们的缓冲区使用了不同的名字。除此之外，我们每次访问**摄像机的名字属性**时，也会最终分配内存，所以我们不想在构建中这样做。

为了解决这两个问题，我们将添加一个`SampleName`字符串属性。如果我们在编辑器中，我们在PrepareBuffer中与缓冲区的名称一起设置它，否则它只是Render Camera字符串的一个简单的常数别名。

```c++
#if UNITY_EDITOR

	…

	string SampleName { get; set; }
	
	…
	
	partial void PrepareBuffer () {
		buffer.name = SampleName = camera.name;
	}

#else

	const string SampleName = bufferName;

#endif
```

```c++
void Setup () {
    context.SetupCameraProperties(camera);
    buffer.ClearRenderTarget(true, true, Color.clear);
    buffer.BeginSample(SampleName);
    ExecuteBuffer();
}

void Submit () {
    buffer.EndSample(SampleName);
    ExecuteBuffer();
    context.Submit();
}
```

我们可以通过检查`profiler`先在编辑器里玩玩看，就知道有什么不同。切换到`Hierarchy mode`，按`GC Alloc`列排序。你会看到两个调用`GC.Alloc`的条目，总共分配了100字节，这是由**检索摄像机名称**引起的。

我们可以通过将**相机名称检索**包装在一个名为`Editor Only`的`profiler sample`中，来明确我们只在编辑器中分配内存，而不是在构建中分配。在这种情况下，我们需要从`UnityEngine.Profiling`命名空间调用`Profiler.BeginSample`和`Profiler.EndSample`。只有`BeginSample`需要被传递名字。

```c#
using UnityEditor;
using UnityEngine;
using UnityEngine.Profiling;
using UnityEngine.Rendering;

partial class CameraRenderer {

	…
	
#if UNITY_EDITOR

	…

	partial void PrepareBuffer () {
		Profiler.BeginSample("Editor Only");
		buffer.name = SampleName = camera.name;
		Profiler.EndSample();
	}

#else

	string SampleName => bufferName;

#endif
}

```

> 看到这里，我终于懂了`profile Sample`的含义：其实就是字面含义，为`profile`生成一个采样点，然后`profile`会对其进行采样分析。

### Layers

摄像机也可以被配置为**只看到某些层上的东西**。这是通过调整他们的==剔除掩码==来实现的。为了看到这个动作，让我们把所有使用**标准着色器**的物体移到`Ignore Raycast layer`。

![image-20210524145131558](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524145131558.png)

将该层排除在**主摄像机的遮盖范围**之外。让它成为`Secondary Camera`看到的唯一一层。

### Clear Flags

我们可以通过调整**第二个被渲染的相机**的**清除标志**来结合两个相机的结果。它们是由`CameraClearFlags`枚举定义的，我们可以通过摄像机的`clearFlags`属性检索到。在清除之前，请在`Setup `中这样做。

```c++
void Setup () {
    context.SetupCameraProperties(camera);
    CameraClearFlags flags = camera.clearFlags;
    buffer.ClearRenderTarget(true, true, Color.clear);
    buffer.BeginSample(SampleName);
    ExecuteBuffer();
}
```

`CameraClearFlags` 枚举定义了四个值。从`1`到`4`，它们是Skybox、Color、Depth和Nothing。**这些实际上并不是独立的标志值**，而是代表一个**递减的清除量**。**深度缓冲区**在所有情况下（除了最后一种情况）都必须被清除，所以**标志值**大多情况下是`Depth`。

```c#
buffer.ClearRenderTarget(
    flags <= CameraClearFlags.Depth, true, Color.clear
);
```

我们只有在标志被设置为`color`时，才真正需要清除**颜色缓冲区**，因为在`Skybox`的情况下，无论如何我们最终都会替换掉之前所有的颜色数据。

```c++
buffer.ClearRenderTarget(
    flags <= CameraClearFlags.Depth,
    flags == CameraClearFlags.Color,
    Color.clear
);
```

而如果我们要清除到一个纯色，我们必须使用**相机的背景色**。但由于我们是在**线性色彩空间**中渲染，我们必须将该颜色转换为线性空间，所以我们最终需要使用`camera.backgroundColor.linear`。在其他情况下，颜色并不重要，所以我们用`Color.clear`就足够了。

```c++
buffer.ClearRenderTarget(
    flags <= CameraClearFlags.Depth,
    flags == CameraClearFlags.Color,
    flags == CameraClearFlags.Color ?
    camera.backgroundColor.linear : Color.clear
);
```

因为**主摄像机**是第一个渲染的，它的**清除标志**应该被设置为`Skybox`或`Color`。当`frame debugger`启用时，我们总是从**清空缓冲区**开始，但这在一般情况下是不能保证的。

**次要摄像机的清除标志**决定了**两个摄像机的渲染**如何被结合起来。在`skybox`或`color`的情况下，之前的结果会被完全替换。当只有**深度**被清除的时候，次要摄像机的渲染和正常的一样，只是它不画天空盒，所以**之前的结果会显示为背景**。当`nothing`时，深度缓冲区被保留，所以`unlit objects`最终会遮挡住无效的物体，就像**它们是由同一个摄像机绘制的**一样。然而，前一个**摄像机所画的透明物体**没有**深度信息**，所以**依然表现的像天空盒**一样。

![image-20210524151416952](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524151416952.png)

通过调整相机的**视口矩形**，也可以将**渲染区域**缩小到整个渲染目标的一小部分。渲染目标的其余部分仍然不受影响。在这种情况下，通过`Hidden/InternalClear`着色器进行清除。**模板缓冲区**被用来限制渲染在视口区域内。

![image-20210524151450045](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524151450045.png)

请注意，每帧渲染一个以上的摄像机意味着删减、设置、排序等工作也要多次进行。每个独特的视点使用一个摄像机通常是最有效的方法。



# 2. Draw Calls

## 2.1 Shaders

为了绘制东西，CPU必须告诉GPU绘制什么和如何绘制。所画的东西通常是一个**网格**。**如何绘制是由着色器定义的**，它是GPU的一组指令。除了网格，着色器还需要额外的信息来完成其工作，包括**物体的变换矩阵**和**材质属性**。

Unity的LW/Universal和HD RP允许你用`Shader Graph`包来设计着色器，它可以为你生成着色器代码。但是我们的**自定义RP**不支持这个，所以我们必须自己编写着色器代码。这使我们能够完全控制和理解着色器的作用。

### Unlit Shader

我们的**第一个着色器**将简单地绘制一个纯色的网格，没有任何照明。一个`shader asset`可以通过`Assets / Create / Shader menu`中的一个选项创建。`Unlit Shader`是最合适的，但我们要从新开始，从创建的shader文件中删除所有的默认代码。将资产命名为`Unlit`，并放在**自定义RP**下的一个新的`Shaders`文件夹中。

着色器的定义就像一个类，但是只有`Shader`关键字，后面跟着一个字符串，用来在材质的`Shader`下拉菜单中为它创建一个条目。让我们使用`Custom RP/Unlit`。接下来是一个代码块，它包含了更多前面有关键词的块。有一个`Properties（属性）块`来定义材质属性，接着是`SubShader`（子着色器）块，里面需要有一个`Pass`块，它定义了渲染的一种方式。

```glsl
Shader "Custom RP/Unlit" {
	
	Properties {}
	
	SubShader {
		
		Pass {}
	}
}
```

**默认的着色器实现**将网格渲染成**纯白色**。材质显示了`render queue`的默认属性，它自动从着色器中获取该属性，并设置为`2000`，这是不透明几何体的默认值。它也有一个切换按钮来启用**双面全局照明**，但这与我们无关。

### HLSL Programs

我们用来编写**着色器代码**的语言是`HLSL`。我们必须把它放在`Pass`块中，在`HLSLPROGRAM`和`ENDHLSL`关键字之间。我们必须这样做，因为在`Pass`块中也有可能放入**其他非HLSL代码**。

```glsl
Pass {
    HLSLPROGRAM
    ENDHLSL
}
```

为了绘制一个网格，`GPU`必须将其所有的三角形**光栅化**，将其转换为**像素数据**。它通过将顶点坐标从**3D空间**转换到**2D可视化空间**，然后填充所有被生成的三角形覆盖的像素。这两个步骤由独立的着色器程序控制，我们必须定义这两个程序。第一个被称为**顶点内核/程序/着色器**，第二个被称为**片段内核/程序/着色器**。一个片段对应于一个显示像素或`texture texel`。

我们必须用一个名字来标识这两个程序，这可以通过`pragma`指令来完成。这些是以`#pragma`开头的单行语句，后面是顶点或片段和相关名称。我们将使用`UnlitPassVertex`和`UnlitPassFragment`

```c#
HLSLPROGRAM
#pragma vertex UnlitPassVertex
#pragma fragment UnlitPassFragment
ENDHLSL
```

> pragma是什么意思？pragma这个词来自希腊语，指的是一个行动，或需要做的事情。在许多编程语言中，它被用来发布特殊的编译器指令。

我们必须用同样的名字编写**HLSL函数**来定义其实现。我们可以直接在pragma指令下面这样做，但我们将把所有的HLSL代码放在一个单独的文件中。具体来说，我们将在同一个资产文件夹中使用一个`UnlitPass.hlsl`文件。我们可以通过添加一个`#include`指令和该文件的相对路径来指示着色器插入该文件的内容。

```c#
HLSLPROGRAM
#pragma vertex UnlitPassVertex
#pragma fragment UnlitPassFragment
#include "Assets\\Custom RP\\Shaders\\UnlitPass.hlsl"
ENDHLSL
```

Unity没有一个方便的菜单选项来创建一个`HLSL`文件，所以你必须做一些事情，比如复制着色文件，将它重命名为`UnlitPass`，将它的文件扩展名更改为HLSL，并清除它的内容。

![image-20210524154911551](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524154911551.png)

### Include Guard

**HLSL文件**用来分组代码，就像**C#类**一样，尽管HLSL没有类的概念。除了代码块的局部作用域，只有一个全局作用域。所以所有的东西都可以在任何地方访问。它在`include指令`的位置插入了文件的全部内容，所以如果你多次包含同一个文件，你会得到重复的代码，这很可能会导致编译器错误。为了防止这种情况，我们将在`UnlitPass.hlsl`中添加一个`include guard`。

可以使用`#define`指令来定义任何标识符，这通常是以大写字母进行的。我们将用它来定义文件顶部的`CUSTOM_UNLIT_PASS_INCLUDED`。

```c#
#define CUSTOM_UNLIT_PASS_INCLUDED
```

这是一个简单宏的例子，它只是定义了一个标识符。如果它存在，那么就意味着我们的文件已经被包含了。所以我们不想再包含它的内容。换句话说，我们只想在它还没有被定义时插入代码。我们可以用`#ifndef`指令来检查。在定义宏之前要这样做。

```c#
#ifndef CUSTOM_UNLIT_PASS_INCLUDED
#define CUSTOM_UNLIT_PASS_INCLUDED
```

在#ifndef之后的所有代码将被跳过，因此如果已经定义了宏，就不会被编译。我们必须通过在文件末尾添加`#endif`指令来终止它的作用域。

```c++
#ifndef CUSTOM_UNLIT_PASS_INCLUDED
#define CUSTOM_UNLIT_PASS_INCLUDED
#endif
```

### Shader Functions

```c++
#ifndef CUSTOM_UNLIT_PASS_INCLUDED
#define CUSTOM_UNLIT_PASS_INCLUDED

void UnlitPassVertex () {}

void UnlitPassFragment () {}

#endif
```

为了产生有效的输出，我们必须使我们的`fragment function`返回一个颜色。我们可以通过`float4(0.0, 0.0, 0.0, 0.0)`来定义纯黑色，但我们也可以写一个单一的`0`，因为单一的值会自动扩展到一个完整的向量。`alpha`值并不重要，因为我们要创建一个不透明的着色器，所以`0`就可以了。

```
float4 UnlitPassFragment () {
	return 0.0;
}
```

> 大多数**移动GPU**支持两种精度类型，`half`的精度更有效率。因此，如果你要为**移动设备**进行优化，尽可能地使用`half`是有意义的。经验法则是只对**位置和纹理坐标**使用浮点，其他都使用`half`，只要结果足够好。当不以移动平台为目标时，精度不是问题，因为GPU总是使用float，即使我们写`half`。在这个教程系列中，我将一贯使用float。还有一种`fixed`类型，但它只被那些你不会针对现代应用的旧硬件真正支持。它通常等同于`half`。

着色编译器将失败，因为我们的函数缺少**语义**。我们必须用返回的值表示我们的意思，因为我们可能产生许多不同含义的数据。在本例中，我们为渲染目标提供了默认的系统值，通过在`UnlitPassFragment`的参数列表后面写一个冒号，然后写上`SV target`来指示。

```c#
float4 UnlitPassFragment () : SV_TARGET {
	return 0.0;
}
```

`UnlitPassVertex`负责转换顶点位置，所以应该返回一个位置。这也是一个`float4`的向量，因为它必须被定义为一个`homogeneous clip space position`，但我们稍后会讨论这个问题。我们再次从`0`开始，在这种情况下，我们必须指出它的含义是`SV_POSITION`。

```c#
float4 UnlitPassVertex () : SV_POSITION {
	return 0.0;
}
```

### Space Transformation

当所有的顶点都被设置为`0`时，网格就会坍缩成一个点，没有任何东西被渲染出来。顶点函数的主要工作是将原始顶点位置转换到正确的空间。当被调用时，如果我们要求的话，该函数会提供可用的顶点数据。我们通过向`UnlitPassVertex`添加参数来做到这一点。我们需要顶点位置，它是在对象空间中定义的，所以我们将它命名为`positionOS`，使用与**Unity的新RPs**相同的约定。这个位置的类型是`float3`，因为它是一个三维点。让我们首先返回它，通过`float4(positionOS, 1.0)`，添加`1`作为第四个需要的分量。

```cc
float4 UnlitPassVertex (float3 positionOS) : SV_POSITION {
	return float4(positionOS, 1.0);
}
```

我们还必须为**输入**添加**语义**，因为顶点数据可以包含不止一个位置。在本例中，我们需要`POSITION`，并在参数名称后直接添加一个颜色。

```c#
float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION {
	return float4(positionOS, 1.0);
}
```

网格又显示出来了，但不正确，因为我们输出的位置是在错误的空间。空间转换需要矩阵，当有东西被画出来时，这些矩阵会被发送到GPU。我们必须将这些矩阵添加到我们的着色器中，但由于它们总是相同的，我们将把Unity提供的标准输入放在一个单独的HLSL文件中，这既是为了保持代码的结构化，也是为了能够在其他着色器中包含这些代码。添加一个`UnityInput.hlsl`文件，把它放在`ShaderLibrary`文件夹中，直接放在`Custom RP`下面，以反映Unity的RP的文件夹结构。

![image-20210524160625963](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524160625963.png)

用一个`CUSTOM_UNITY_INPUT_INCLUDED` include guard开始文件，然后在全局范围内定义一个名为`unity_ObjectToWorld`的`float4x4`矩阵。它在每次绘制时被GPU设置一次，在绘制过程中的所有顶点和片段函数的调用中保持不变

```c#
#ifndef CUSTOM_UNITY_INPUT_INCLUDED
#define CUSTOM_UNITY_INPUT_INCLUDED

float4x4 unity_ObjectToWorld;

#endif
```

我们可以使用矩阵来从**物体空间**转换到**世界空间**。由于这是常见的功能，让我们为它创建一个函数并把它放在另一个文件中，这次是同一个`ShaderLibrary`文件夹中的`Common.hlsl`。我们把`UnityInput`放在那里，然后声明一个`TransformObjectToWorld`函数，用一个`float3`作为输入和输出。

```
#ifndef CUSTOM_COMMON_INCLUDED
#define CUSTOM_COMMON_INCLUDED

#include "UnityInput.hlsl"

float3 TransformObjectToWorld (float3 positionOS) {
	return 0.0;
}
	
#endif
```

空间转换是通过调用mul函数（一个矩阵和一个矢量）来完成的。在这种情况下，我们确实需要一个4D向量，但由于它的第四个分量总是1，我们可以通过使用float4(positionOS, 1.0)来自己添加它。结果还是一个4D向量，其第四个分量总是1。我们可以通过访问该向量的xyz属性来提取前三个分量：

```cc
float3 TransformObjectToWorld (float3 positionOS) {
	return mul(unity_ObjectToWorld, float4(positionOS, 1.0)).xyz;
}
```

首先将Common.hlsl包含在函数的正上方。因为它存在于不同的文件夹中，我们可以通过相对路径 `.../ShaderLibrary/Common.hlsl`来获得它。然后使用TransformObjectToWorld来计算一个positionWS变量，并返回它。

```c#
#include "../ShaderLibrary/Common.hlsl"

float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION {
	float3 positionWS = TransformObjectToWorld(positionOS.xyz);
	return float4(positionWS, 1.0);
}
```

同样的过程：

```c++
float4x4 unity_ObjectToWorld;

float4x4 unity_MatrixVP;
```

```c#
float3 TransformObjectToWorld (float3 positionOS) {
	return mul(unity_ObjectToWorld, float4(positionOS, 1.0)).xyz;
}

float4 TransformWorldToHClip (float3 positionWS) {
	return mul(unity_MatrixVP, float4(positionWS, 1.0));
}
```

```c#
float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION {
	float3 positionWS = TransformObjectToWorld(positionOS.xyz);
	return TransformWorldToHClip(positionWS);
}
```

![image-20210524162537506](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524162537506.png)

### Core Library

我们刚才定义的两个函数非常常见，以至于它们也被包含在`Core RP Pipeline package`。核心库定义了更多有用的和必要的东西，所以让我们安装那个包，删除我们自己的定义，而是包括相关的文件，在这里是`Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl`。

这就无法编译了，因为`SpaceTransforms.hlsl`中的代码并没有假定`unity_ObjectToWorld`的存在。相反，它希望相关的矩阵被一个宏定义为`UNITY_MATRIX_M`，所以让我们在包含文件之前，在单独一行写`#define UNITY_MATRIX_M unity_ObjectToWorld`。之后，所有出现的`UNITY_MATRIX_M`将被`unity_ObjectToWorld`取代。

```c#
#define UNITY_MATRIX_M unity_ObjectToWorld

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"
```

同理：

```c#
#define UNITY_MATRIX_I_M unity_WorldToObject
#define UNITY_MATRIX_V unity_MatrixV
#define UNITY_MATRIX_VP unity_MatrixVP
#define UNITY_MATRIX_P glstate_matrix_projection
```

将额外的矩阵也添加到`UnityInput`中。

```c#
float4x4 unity_ObjectToWorld;
float4x4 unity_WorldToObject;

float4x4 unity_MatrixVP;
float4x4 unity_MatrixV;
float4x4 glstate_matrix_projection;
```

最后缺少的是一个矩阵以外的东西。它是`unity_WorldTransformParams`，它包含了一些我们在这里不需要的变换信息。它是一个定义为`real4`的向量，它本身不是一个有效的类型，而是对float4或half4的别名，取决于目标平台。

```c#
float4x4 unity_ObjectToWorld;
float4x4 unity_WorldToObject;
real4 unity_WorldTransformParams;
```

这个别名和其他很多**基本的宏**都是按**图形API**定义的，我们可以通过包含`Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl`来获得所有这些。

```c++
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "UnityInput.hlsl"
```

> com.unity.render-pipelines.core：在包管理页面，进行下载

### Color

为了使配置**每个材质的颜色**成为可能，我们必须把它定义为一个`uniform`值。在include指令下面，在UnlitPassVertex函数之前做这个。我们需要一个float4，并将其命名为_BaseColor。**前面的下划线**是表示它代表一个材质属性的标准方式。在`UnlitPassFragment`中返回这个值，而不是一个硬编码的颜色。

```c#
#include "../ShaderLibrary/Common.hlsl"

float4 _BaseColor;

float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION {
	float3 positionWS = TransformObjectToWorld(positionOS);
	return TransformWorldToHClip(positionWS);
}

float4 UnlitPassFragment () : SV_TARGET {
	return _BaseColor;
}
```

我们回到了黑色，因为默认值是0。为了将其与材质联系起来，我们必须将`_BaseColor`添加到`Unlit shader`的属性块中。属性名后面必须有一个在`inspector `中使用的字符串和一个`Color`类型标识符，就像给一个方法提供参数一样。

```c++
Properties {
	_BaseColor("Color", Color) = (1.0, 1.0, 1.0, 1.0)
}
```



## 2.2 Batching

每一个`Draw Call`都需要**CPU**和**GPU**之间的通信。如果有大量的数据需要发送给GPU，那么它可能会因为等待而浪费时间。而当CPU忙于发送数据时，它就不能做其他事情。这两个问题都会降低==帧率==。目前，我们的方法很直接：每个对象都有自己的`Draw Call`。这是最糟糕的方法。

作为一个例子，我做了一个有76个球体的场景，每个球体使用四种材质中的一种：红、绿、黄、蓝。渲染时需要调用`78`次，其中76次用于`球体`，一次用于skybox，还有一次用于清除**渲染目标**。

如果游戏窗口的 "统计`Stats` "面板，你可以看到渲染帧所需的概况。这里有趣的事实是，它显示了77个`batching`——不考虑清除。

![image-20210524170208861](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524170208861.png)

### SRP Batcher

批处理`Batching`是组合`Draw Call`的过程，减少CPU和GPU之间的通信时间。最简单的方法是==启用SRP批处理程序==。然而，这只适用于兼容的着色器，而我们的Unlit着色器不是。你可以通过在检查器中选择它来验证这一点。有一个**SRP批处理程序的行**表示不兼容，在它下面给出了一个原因。

![image-20210524170558659](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524170558659.png)

:star:与其说**SRP批处理**减少了`Draw Call`的数量，不如说是使其更加精简。==它在GPU上缓存了材质属性==，所以它们不必在每次`Draw Call`时都被发送。这既减少了必须传达的数据量，也减少了CPU在每次绘制调用时必须做的工作。但是，这只有在**着色器**遵守`uniform data`的严格结构时才有效。

**所有的材质属性都必须被定义在一个具体的内存缓冲区内，而不是在全局层面**。这是通过将**_BaseColor声明**包裹在一个带有`UnityPerMaterial`名称的`cbuffer`块中来实现的。这就像一个结构声明，但必须用分号来结束。它通过将_**BaseColor**放在一个特定的`Constant buffers`中来隔离 `_BaseColor`，尽管它仍然可以在全局级别上访问。

```c#
cbuffer UnityPerMaterial {
	float _BaseColor;
};
```

`Constant buffers`并不支持所有的平台，比如OpenGL ES 2.0，所以我们可以使用从**核心RP库**中包含的`CBUFFER_START`和`CBUFFER_END`宏，而不是直接使用cbuffer。第一个宏把**缓冲区名称**作为参数，就像它是一个函数一样。在这种情况下，我们最终得到的结果和之前的完全一样，只是在不支持`cbuffer`的平台上，`cbuffer`代码将不存在。

```c++
CBUFFER_START(UnityPerMaterial)
	float4 _BaseColor;
CBUFFER_END
```

对于`unity_ObjectToWorld`、`unity_WorldToObject`和`unity_WorldTransformParams`，我们也必须这样做，只不过它们必须被分组在`UnityPerDraw`缓冲区中。

```c#
CBUFFER_START(UnityPerDraw)
	float4x4 unity_ObjectToWorld;
	float4x4 unity_WorldToObject;
	real4 unity_WorldTransformParams;
CBUFFER_END
```

在这种情况下，如果我们使用其中的一个，我们就需要定义特定的数值组。对于`transformation`组，我们还需要包括`float4 unity_LODFade`，尽管我们并没有使用它。具体的顺序并不重要，但是Unity把它直接放在unity_WorldToObject之后，所以我们也要这样做。

```c#
CBUFFER_START(UnityPerDraw)
	float4x4 unity_ObjectToWorld;
	float4x4 unity_WorldToObject;
	float4 unity_LODFade;
	real4 unity_WorldTransformParams;
CBUFFER_END
```

![image-20210524172230702](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524172230702.png)

在**着色器兼容**的情况下，下一步是启用==SRP批处理程序==，这可以通过设置`GraphicsSettings.useScriptableRenderPipelineBatching`为`true`来完成。我们只需要做一次，所以让我们在**创建管道实例**时进行，为`CustomRenderPipeline`添加一个构造方法。

```c#
public CustomRenderPipeline () {
    GraphicsSettings.useScriptableRenderPipelineBatching = true;
}
```

![image-20210524172639131](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524172639131.png)

`Frame Debugger`现在在`RenderLoopNewBatcher.Draw`下显示了一个**单一的SRP批处理条目**，不过请记住，**这不是一个单一的绘制调用，而是一个优化的序列**。

### Many Colors

尽管我们使用了四个材质，但我们得到了一个`batch`。这是因为它们的所有数据都被缓存在GPU上，每个绘制调用只需要包含一个偏移。**唯一的限制是每个材质的内存布局必须是相同的**，这是因为我们对所有的材质使用相同的着色器，每个材质只包含一个颜色属性。Unity不会比较材质的**确切内存布局**，它只是将**使用完全相同的着色器变量的绘制调用**进行分组。

如果我们想要一些不同的颜色，这样做很好，但是如果我们想要给每个球体提供自己的颜色，那么我们就必须创建更多的材质。如果我们可以为每个对象设置颜色，那就更方便了。这在默认情况下是不可能的，但我们可以通过创建一个**自定义组件类型**来支持它。把它命名为`PerObjectMaterialProperties`。因为它是一个例子，我把它放在`*Custom RP*`下的`Examples`文件夹里。

我们的想法是，一个游戏对象可以有一个`PerObjectMaterialProperties`组件附在它身上，它有一个`Base Color`配置选项，这将被用来为它设置**_BaseColor材质属性**。它需要知道**着色器属性的标识符**，我们可以通过`Shader.PropertyToID`检索并存储在一个静态变量中，就像我们在`CameraRenderer`中对`shader pass`标识符所做的那样，尽管在这种情况下它是一个整数。

```c++
using UnityEngine;

[DisallowMultipleComponent]
public class PerObjectMaterialProperties : MonoBehaviour {
	
	static int baseColorId = Shader.PropertyToID("_BaseColor");
	
	[SerializeField]
	Color baseColor = Color.white;
}
```

设置每个对象的**材质属性**是通过一个`MaterialPropertyBlock`对象完成的：

```cc
static MaterialPropertyBlock block;
```

创建一个新的块，然后用**属性标识符**和**颜色**调用`SetColor`，然后通过`SetPropertyBlock`将该块应用到游戏对象的`Renderer`组件上，从而复制其设置。在`OnValidate`中这样做，这样结果会立即显示在编辑器中。

```c#
void OnValidate () {
    if (block == null) {
    	block = new MaterialPropertyBlock();
    }
    block.SetColor(baseColorId, baseColor);
    GetComponent<Renderer>().SetPropertyBlock(block);
}
```

> OnValidate：在组件加载或修改时，就会触发

不幸的是，**SRP批处理程序**不能处理`per-object material properties`。因此，这`24`个球体又回到了每个球体的常规绘制调用，由于排序的原因，可能会将其他球体也分成多个批次。

此外，`OnValidate`不会在`build`中被调用。要在那里显示单独的颜色，我们还必须在Awake中应用它们，这可以通过在那里调用`OnValidate`来实现。

```c#
void Awake () {
	OnValidate();
}
```

### GPU Instancing

还有另一种方法可以整合`draw call`，被称为==GPU实例化==，其工作方式是对**具有相同网格的多个对象**同时发出一个`draw call`。CPU收集每个对象的`tranform`和**材质属性**，并将其放入数组，然后发送给GPU。然后，GPU遍历所有条目，并按照**它们被提供的顺序**进行渲染。

因为`GPU Instancing`需要通过数组提供数据，所以我们的着色器目前不支持它。要实现这一点，首先要在着色器的`Pass`块中的顶点和片段pragmas之上添加`#pragma multi_compile_instancing`指令。

```c#
#pragma multi_compile_instancing
#pragma vertex UnlitPassVertex
#pragma fragment UnlitPassFragment
```

这将使Unity生成两个着色器变体，一个有GPU实例支持，一个没有GPU实例支持。在`material inspector`中也出现了一个切换选项，它允许我们为材质选择版本。

![image-20210524200057783](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524200057783.png)

支持**GPU实例化**需要改变方法，为此我们必须加入`core shader library`的**UnityInstancing.hlsl**文件。这必须在定义了UNITY_MATRIX_M和其他宏之后，在include SpaceTransforms.hlsl之前完成。

```c#
#define UNITY_MATRIX_P glstate_matrix_projection

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/UnityInstancing.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"
```

`UnityInstancing.hlsl`所做的是**重新定义这些宏**，以代替访问**实例数据数组**。但是为了使其发挥作用，**它需要知道当前正在渲染的对象的索引**。这个索引是通过顶点数据提供的，所以我们必须让它可用。`UnityInstancing.hlsl`定义了一些宏来使之变得简单，但**它们假定我们的顶点函数有一个结构参数**。

我们可以声明一个结构（就像cbuffer一样）并将其作为一个函数的输入参数。我们还可以在结构内部定义语义。这种方法的好处是，它比长长的参数列表更容易辨认。因此，将`UnlitPassVertex`的**positionOS参数**包裹在一个**Attributes结构**中，代表顶点输入数据：

```c#
struct Attributes {
	float3 positionOS : POSITION;
};

float4 UnlitPassVertex (Attributes input) : SV_POSITION {
	float3 positionWS = TransformObjectToWorld(input.positionOS);
	return TransformWorldToHClip(positionWS);
}
```

当使用**GPU实例化**时，**对象索引**也可以作为一个**顶点属性**使用。我们可以添加它，只需将`UNITY_VERTEX_INPUT_INSTANCE_ID`放在**Attributes**里面。

```c#
struct Attributes {
	float3 positionOS : POSITION;
	UNITY_VERTEX_INPUT_INSTANCE_ID
};
```

接下来，在`UnlitPassVertex`的开头添加`UNITY_SETUP_INSTANCE_ID(input); `。这将**从输入中提取索引**，并将其存储在**其他实例化宏所依赖的**全局静态变量中。

```c#
float4 UnlitPassVertex (Attributes input) : SV_POSITION {
	UNITY_SETUP_INSTANCE_ID(input);
	float3 positionWS = TransformObjectToWorld(input.positionOS);
	return TransformWorldToHClip(positionWS);
}
```

这足以让**GPU实例化**工作，尽管`SRP batcher`优先。但是我们还不支持**材质数据**。为了增加这一点，我们必须在需要时用一个**数组引用**替换`_BaseColor`。这是通过用`UNITY_INSTANCING_BUFFER_START`替换`CBUFFER_START`，用`UNITY_INSTANCING_BUFFER_END`替换`CBUFFER_END`来实现的，也需要一个参数。

```c#
//CBUFFER_START(UnityPerMaterial)
//	float4 _BaseColor;
//CBUFFER_END

UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	float4 _BaseColor;
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)
```

然后用`UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor)`替换`_BaseColor`的定义。

```c#
UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	//	float4 _BaseColor;
	UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor)
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)
```

当使用**实例化**时，我们现在还必须使**实例索引**在`UnlitPassFragment`中可用。为了方便起见，我们将使用一个结构来让`UnlitPassVertex`同时输出**位置和索引**，使用`UNITY_TRANSFER_INSTANCE_ID(input, output);`来复制**存在的索引**。我们像Unity那样将这个结构命名为`Varyings`，因为它包含的数据可以在同一个三角形的`fragment`之间变化。

```c#
struct Varyings {
	float4 positionCS : SV_POSITION;
	UNITY_VERTEX_INPUT_INSTANCE_ID
};

Varyings UnlitPassVertex (Attributes input) { //: SV_POSITION {
	Varyings output;
	UNITY_SETUP_INSTANCE_ID(input);
	UNITY_TRANSFER_INSTANCE_ID(input, output);
	float3 positionWS = TransformObjectToWorld(input.positionOS);
	output.positionCS = TransformWorldToHClip(positionWS);
	return output;
}
```

将此结构作为参数添加到 `UnlitPassFragment `中。然后像以前一样使用`UNITY_SETUP_INSTANCE_ID`来使索引可用。现在必须通过`UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor)`访问材质属性。

```c#
float4 UnlitPassFragment (Varyings input) : SV_TARGET {
	UNITY_SETUP_INSTANCE_ID(input);
	return UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor);
}
```

减少了`Draw Call`的数量。**GPU实例化**只对**共享相同材质的对象**有效。因为它们覆盖了材质的颜色，所以它们都可以使用相同的材质，然后允许它们在一个批次中被绘制。

![image-20210524202736144](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210524202736144.png)

请注意，根据**目标平台**和提供的数据，对`batch size`是有限制的。如果你超过了这个限制，那么最终会得到不止一批。此外，如果有多种材料在使用，分类仍然可以分割`batch`。

### Drawing Many Instanced Meshes

当数百个对象可以在一次`Draw Call`中组合起来时，**GPU实例化**就成为一个显著的优势。但是用手编辑场景中的这么多对象并不实际。所以让我们随机生成。创建一个`MeshBall`组件，当它被唤醒的时候会产生很多对象。让它缓存`_BaseColor`着色器属性，并为网格和材质添加配置选项，**这些选项必须支持实例化**。

```c#
using UnityEngine;

public class MeshBall : MonoBehaviour {

	static int baseColorId = Shader.PropertyToID("_BaseColor");

	[SerializeField]
	Mesh mesh = default;

	[SerializeField]
	Material material = default;
}
```

添加一个物体，加上这个组件，进行如下配置：

![image-20210525135931183](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525135931183.png)

我们可以生成许多新的游戏对象，但我们没有必要这样做。相反，我们将填充一个**变换矩阵**和**颜色**的数组，并告诉`GPU`用这些来渲染一个网格。**这就是GPU实例化最有用的地方**。我们可以一次性提供多达`1023`个实例，所以让我们添加具有这个长度的数组，以及一个我们需要传递颜色数据的`MaterialPropertyBlock`。在这种情况下，颜色数组的元素类型必须是`Vector4`。

```c#
Matrix4x4[] matrices = new Matrix4x4[1023];
Vector4[] baseColors = new Vector4[1023];

MaterialPropertyBlock block;
```

创建一个`Awake`，用半径为`10`的**球体内的随机位置**和**随机的RGB颜色数据**来填充数组

```c#
void Awake () {
    for (int i = 0; i < matrices.Length; i++) {
        matrices[i] = Matrix4x4.TRS(
        Random.insideUnitSphere * 10f, Quaternion.identity, Vector3.one
    	);
        baseColors[i] =
        new Vector4(Random.value, Random.value, Random.value, 1f);
    }
}
```

在`Update`中，我们创建一个新的块，并对它调用SetVectorArray来配置颜色。之后调用Graphics.DrawMeshInstanced，将网格、子网格索引为0、材质、矩阵阵列、元素数量和属性块作为参数。我们在这里设置了这个块，这样**网格球**就能在**热重载**中存活下来。

```c++
void Update () {
    if (block == null) {
        block = new MaterialPropertyBlock();
        block.SetVectorArray(baseColorId, baseColors);
    }
    Graphics.DrawMeshInstanced(mesh, 0, material, matrices, 1023, block);
}
```

![image-20210525141020146](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525141020146.png)

现在进入游戏模式将产生一个密集的球体群。**需要多少次绘制调用取决于平台**，因为最大缓冲区大小不同。在我的例子中，它需要`2`次DC来渲染。

请注意，各个网格的绘制顺序与我们提供数据的顺序相同。除此之外，**没有任何排序或剔除**，尽管整个批处理一旦超出视图范围就会消失。

### Dynamic Batching

还有一种减少绘制调用的方法，被称为==动态批处理==。这是一种**古老的技术**，它将**共享相同材质的多个小网格**组合成**一个较大的网格**，然后绘制。当使用`per-object material properties`时，这种方法也不起作用。

较大的网格是按需生成的，所以它只对小网格可行。球体太大，但它对立方体是有效的。要看到它的作用，请禁用GPU实例化，并在`CameraRenderer.DrawVisibleGeometry`中设置`enableDynamicBatching`为`true`。

```c#
var drawingSettings = new DrawingSettings(
    unlitShaderTagId, sortingSettings
) {
    enableDynamicBatching = true,
    enableInstancing = false
};
```

同时禁用**SRP批处理程序**`SRP batcher`，因为它是优先的。

```c#
GraphicsSettings.useScriptableRenderPipelineBatching = false;
```

![image-20210525142725766](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525142725766.png)

一般来说，**GPU实例化**比**动态批处理**效果更好。这种方法也有一些注意事项，例如，当涉及到不同比例时，较大的网格的法向量不能保证是单位长度的。另外，绘制顺序也会发生变化，因为现在是单个网格而不是多个。

**还有一种静态批处理**，它的工作原理与此类似，对于被标记为`batching-static`的对象，提前进行批处理。除了需要更多的内存和存储，它没有任何注意事项。RP不关注这个，所以我们不必考虑这个问题。

### Configuring Batching

哪种方法是最好的，可能会有所不同，所以让我们让它变得**可配置**。首先，添加**布尔参数**来控制**动态批处理**和**GUI实例化**是否被用于`DrawVisibleGeometry`，而不是硬编码。

```c++
void DrawVisibleGeometry (bool useDynamicBatching, bool useGPUInstancing) {
    var sortingSettings = new SortingSettings(camera) {
        criteria = SortingCriteria.CommonOpaque
    };
    var drawingSettings = new DrawingSettings(
        unlitShaderTagId, sortingSettings
    ) {
        enableDynamicBatching = useDynamicBatching,
        enableInstancing = useGPUInstancing
    };
    …
}
```

```c#
public void Render (
    ScriptableRenderContext context, Camera camera,
    bool useDynamicBatching, bool useGPUInstancing
) {
    …
    DrawVisibleGeometry(useDynamicBatching, useGPUInstancing);
    …
}
```

`CustomRenderPipeline`将通过字段跟踪选项，在其构造方法中设置，并在Render中传递它们。同时在构造函数中为**SRP批处理程序**添加一个**布尔参数**，而不是总是启用它。

```c#
bool useDynamicBatching, useGPUInstancing;

public CustomRenderPipeline (
    bool useDynamicBatching, bool useGPUInstancing, bool useSRPBatcher
) {
    this.useDynamicBatching = useDynamicBatching;
    this.useGPUInstancing = useGPUInstancing;
    GraphicsSettings.useScriptableRenderPipelineBatching = useSRPBatcher;
}

protected override void Render (
    ScriptableRenderContext context, Camera[] cameras
) {
    foreach (Camera camera in cameras) {
        renderer.Render(
            context, camera, useDynamicBatching, useGPUInstancing
        );
    }
}
```

最后，将这三个选项作为**配置字段**添加到`CustomRenderPipelineAsset`中，将它们传递给`CreatePipeline`的构造函数调用。

```c#
[SerializeField]
bool useDynamicBatching = true, useGPUInstancing = true, useSRPBatcher = true;

protected override RenderPipeline CreatePipeline () {
    return new CustomRenderPipeline(
        useDynamicBatching, useGPUInstancing, useSRPBatcher
    );
}
```

现在可以改变RP所使用的方法了。切换一个选项将立即生效，因为**Unity编辑器**将在检测到资产被改变时，创建一个**新的RP实例**。



## 2.3 Transparency

我们可以改变颜色的`alpha`分量，这通常表示透明度，但目前没有效果。我们也可以将**渲染队列**设置为`Transparent`，但这只能改变物体被绘制的时间和顺序，而**不能改变绘制方式**。

我们不需要写一个**单独的着色器**来支持**透明材质**。只要一点工作，我们的`Unlit着色器`就可以同时支持**不透明**和**透明**渲染。

### Blend Modes

不透明和透明渲染的主要区别在于：我们是替换之前绘制的东西，还是与**之前的结果**结合起来产生透视的效果。我们可以通过设置**源和目标混合模式**来控制这一点。这里的==源==指的是**现在被绘制的东西**，而==目的==指的是**之前被绘制的东西**，以及结果最终会在哪里出现。为此添加两个着色器属性。·_SrcBlend和_DstBlend。它们是混合模式的枚举，默认情况下，源点设置为`1`，目的设置为`0`。

```c#
Properties {
    _BaseColor("Color", Color) = (1.0, 1.0, 1.0, 1.0)
        _SrcBlend ("Src Blend", Float) = 1
        _DstBlend ("Dst Blend", Float) = 0
}
```

为了使编辑更容易，我们可以向属性中添加**Enum属性**，

```c#
[Enum(UnityEngine.Rendering.BlendMode)] _SrcBlend ("Src Blend", Float) = 1
[Enum(UnityEngine.Rendering.BlendMode)] _DstBlend ("Dst Blend", Float) = 0
```

标准透明度的**源混合模式是SrcAlpha**，这意味着渲染的颜色的RGB分量要乘以它的alpha分量。所以α值越低，它就越弱。目标混合模式则被设置为相反的模式：`OneMinusSrcAlpha`，得出的总权重为1。

==混合模式==可以在`Pass`块中用`Blend`语句定义。我们想使用**着色器属性**，在这里，我们可以通过把它们放在`[]`里来访问。这是个老语法。

```c#
Pass {
    Blend [_SrcBlend] [_DstBlend]

    HLSLPROGRAM
    …
    ENDHLSL
}
```

![image-20210525145801270](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525145801270.png)

> 这里我们根本没有在HLSL中用到这两个属性，所以他们也不需要像`_baseColor`一样放在那几个宏中间，去实现GPU实例化。

### Not Writing Depth

**透明渲染**通常不向**深度缓冲区**写入（因为它不会从中受益，甚至可能产生不想要的结果）。我们可以通过`ZWrite`语句来控制深度是否被写入。我们可以使用着色器属性，这次是`_ZWrite`。用一个==自定义的Enum==(Off, 0, On, 1)属性来定义着色器属性，以创建一个默认值为0和1的**开关切换**。

```c#
[Enum(UnityEngine.Rendering.BlendMode)] _SrcBlend ("Src Blend", Float) = 1
[Enum(UnityEngine.Rendering.BlendMode)] _DstBlend ("Dst Blend", Float) = 0
[Enum(Off, 0, On, 1)] _ZWrite ("Z Write", Float) = 1

Blend [_SrcBlend] [_DstBlend]
ZWrite [_ZWrite]
```

![image-20210525150310092](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525150310092.png)

### Texturing

之前我们使用**alpha贴图**来创建一个**非均匀的半透明材质**。让我们通过给着色器添加一个`_BaseMap`纹理属性来支持它。在这种情况下，类型是`2D`，我们将使用**Unity的标准白色纹理**作为默认。另外，我们必须用一个**空的代码块**来**结束纹理属性**（`end the texture property `）。

```c#
_BaseMap("Texture", 2D) = "white" {}
_BaseColor("Color", Color) = (1.0, 1.0, 1.0, 1.0)
```

纹理必须被上传到**GPU内存**中，Unity为我们做了这个。着色器需要一个**相关纹理的句柄**，我们可以像定义一个`uniform`一样定义这个句柄，只不过我们使用`TEXTURE2D`宏，把名称作为一个参数。我们还需要为纹理定义一个采样器状态，考虑到它的`wrap`和`filter`模式，控制它应该如何采样。这是用`SAMPLER`宏来完成的，就像`TEXTURE2D`一样，但名字前加了`Sampler`。这与**Unity自动提供的采样器状态**的名称相匹配。

**纹理**和**采样器状态**是着色器资源。**它们不能为每个实例提供，必须在全局范围内声明**。在`UnlitPass.hlsl`的**着色器属性**之前做这个。

```c#
TEXTURE2D(_BaseMap);
SAMPLER(sampler_BaseMap);

UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor)
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)
```

除此之外，Unity还通过一个`float4`提供了**纹理的平铺和偏移**，它的名字与纹理属性相同，但附加了`_ST`，它代表缩放和平移。这个属性应该是`UnityPerMaterial`缓冲区的一部分，所以它可以为**每个实例**设置。

```c#
UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	UNITY_DEFINE_INSTANCED_PROP(float4, _BaseMap_ST)
	UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor)
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)
```

为了对纹理进行采样，我们需要**纹理坐标**，它是**顶点属性**的一部分。具体来说，我们需要一对坐标，还可能有更多。这可以通过向`Attributes`添加一个具有`TEXCOORD0`含义的`float2`字段来实现。由于这是我们的基础贴图，而纹理空间的尺寸普遍被命名为U和V，我们将其命名为`baseUV`。

```c++
struct Attributes {
	float3 positionOS : POSITION;
	float2 baseUV : TEXCOORD0;
	UNITY_VERTEX_INPUT_INSTANCE_ID
};
```

我们需要将坐标传递给**fragment函数**。所以也要在`Varyings`中添加`float2 baseUV`。这一次我们不需要添加特殊的含义，它只是我们传递的数据，不需要GPU的特别关注。然而，我们仍然要给它附加一些意义。我们可以应用**任何不用的标识符**，让我们简单地使用`VAR_BASE_UV`。

```c#
struct Varyings {
	float4 positionCS : SV_POSITION;
	float2 baseUV : VAR_BASE_UV;
	UNITY_VERTEX_INPUT_INSTANCE_ID
};
```

当我们复制`UnlitPassVertex`中的坐标时，我们也可以应用存储在`_BaseMap_ST`中的比例和偏移。这样我们就可以按顶点而不是按片断来做。比例存储在`XY`，偏移存储在`ZW`。

```c#
Varyings UnlitPassVertex (Attributes input) {
	…

	float4 baseST = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseMap_ST);
	output.baseUV = input.baseUV * baseST.xy + baseST.zw;
	return output;
}
```

现在**UV坐标**可供`UnlitPassFragment`使用。在这里对纹理进行采样，使用`SAMPLE_TEXTURE2D`宏，将纹理、采样器状态和坐标作为参数。最终的颜色是通过乘法将纹理和`uniform color`结合起来。

```c#
float4 UnlitPassFragment (Varyings input) : SV_TARGET {
	UNITY_SETUP_INSTANCE_ID(input);
	float4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, input.baseUV);
	float4 baseColor = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor);
	return baseMap * baseColor;
}
```

### Alpha Clipping

另一种**看穿表面的方法**是在它们身上==开洞==。着色器也可以做到这一点，通过丢弃`fragment`。这就产生了坚硬的边缘，而不是我们目前看到的平滑过渡。这种技术被称为` alpha clipping`。做到这一点的通常方法是定义一个**截止阈值**。α值低于这个阈值的片段将被丢弃，而其他的则被保留。

添加一个`_Cutoff`属性，默认设置为`0.5`。

```c#
_BaseColor("Color", Color) = (1.0, 1.0, 1.0, 1.0)
_Cutoff ("Alpha Cutoff", Range(0.0, 1.0)) = 0.5
```

```c#
UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor)
UNITY_DEFINE_INSTANCED_PROP(float, _Cutoff)
```

我们可以通过调用`UnlitPassFragment`中的**clip函数**来丢弃片段：

```c#
float4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, input.baseUV);
float4 baseColor = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor);
float4 base = baseMap * baseColor;
clip(base.a - UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _Cutoff));
return base;
```

![image-20210525154359335](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525154359335.png)

一个材质通常使用**透明度混合**或` alpha clipping`，而不是同时使用两者。**一个典型的`clip`材质是完全不透明的，除了被丢弃的片段之外，它会向深度缓冲区写入数据**。它使用==AlphaTest渲染队列==，这意味着它在所有完全不透明的对象之后被渲染。之所以这样做，是因为丢弃`fragment`会使一些**GPU优化**变得不可能，因为不能再假设三角形完全覆盖它们后面的东西。通过先绘制完全不透明的物体，它们最终可能会覆盖部分alpha-clipped物体，这样就不需要处理它们隐藏的片段了。

![image-20210525154553031](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525154553031.png)

但是，为了使这种优化发挥作用，我们必须确保`clip`只在需要的时候使用。我们将通过添加一个`feature toggle shader property`来做到这一点。这是一个默认设置为`0`的Float属性，有一个控制**着色器关键字**的Toggle属性。

```c#
_Cutoff ("Alpha Cutoff", Range(0.0, 1.0)) = 0.5
[Toggle(_CLIPPING)] _Clipping ("Alpha Clipping", Float) = 0
```

### Shader Features

启用该`toggle`将把`_CLIPPING`**关键字**添加到**材质的活动关键字列表**中，而禁用将删除它。但这本身并没有什么作用。我们必须告诉Unity根据**关键字是否被定义**来**编译不同版本的着色器**。我们通过在其`Pass`中的指令中添加`#pragma shader_feature _CLIPPING`来做到这一点。

```c#
#pragma shader_feature _CLIPPING
#pragma multi_compile_instancing
```

现在，Unity将编译我们的着色器代码，不管是否定义了`_CLIPPING`。它将生成一个或两个变体，这取决于我们如何配置我们的材质。所以我们可以让我们的代码以定义为条件，就像include guards一样，但在这种情况下，我们只想在定义了`_CLIPPING`的情况下包含`clipping`。我们可以为此使用`#ifdef _CLIPPING`，但我更喜欢`#if defined(_CLIPPING)`。

```c#
#if defined(_CLIPPING)
	clip(base.a - UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _Cutoff));
#endif
```

### Cutoff Per Object

由于`cutoff`是UnityPerMaterial缓冲区的一部分，它可以**按实例**进行配置。所以让我们把这个功能添加到`PerObjectMaterialProperties`中。它与颜色的工作原理相同，只是我们需要在**属性块**上调用`SetFloat`。

```c#
static int baseColorId = Shader.PropertyToID("_BaseColor");
static int cutoffId = Shader.PropertyToID("_Cutoff");

static MaterialPropertyBlock block;

[SerializeField]
Color baseColor = Color.white;

[SerializeField, Range(0f, 1f)]
float cutoff = 0.5f;

…

void OnValidate () {
    …
    block.SetColor(baseColorId, baseColor);
    block.SetFloat(cutoffId, cutoff);
    GetComponent<Renderer>().SetPropertyBlock(block);
}
```

### Ball of Alpha-Clipped Spheres

`MeshBall`的情况也是如此。

```c#
matrices[i] = Matrix4x4.TRS(
    Random.insideUnitSphere * 10f,
    Quaternion.Euler(
        Random.value * 360f, Random.value * 360f, Random.value * 360f
    ),
    Vector3.one * Random.Range(0.5f, 1.5f)
);
baseColors[i] =
    new Vector4(
    Random.value, Random.value, Random.value,
    Random.Range(0.5f, 1f)
);
```

![image-20210525160942054](C:\Users\xueyaojiang\Desktop\JMX\UnityStudy\Catlike Coding教程\Custom SRP\C1.assets\image-20210525160942054.png)

